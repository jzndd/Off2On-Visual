defaults:
  - _self_
  - env: metaworld
  - agent: metaworld

debug_mode: false
seed: 0
task: hammer-v2

is_sparse_reward: True
is_whole_traj: False

save_data: False
only_bc: False
img_size: 96

hydra:
  run:
    dir: outputs/${now:%Y.%m.%d}/${now:%H%M%S}_${task}_${wandb.group}_${wandb.name}
  job:
    chdir: True  # Don't change the working directory to the output folder

common:
  devices: [6]  # int, list of int, cpu, or all 
  seed: ${seed}
  resume: False # do not modify, set by scripts/resume.sh only.

ac_type: ppo  # Optional[ppo, bc, iql]
offline_buffer: /data/jzn/workspace/ppo_alg/Off2On-Visual/data/${task}_${img_size}/expert_rb_with_reward.pkl
expert_rb_dir: ${offline_buffer}

bc_ckpt_dir: /data/jzn/workspace/ppo_alg/Off2On-Visual/ckpt/${task}_${ac_type}_2D_${agent.ppo_cfg.adv_compute_mode}_seed${common.seed}/bc.pth

wandb:
  mode: online
  project: offline2online-metaworld_${task}
  entity: jzndd1
  name: seed${seed}_bs${actor_critic.training.batch_size}_minibs${agent.ppo_cfg.mini_batch_size}
  group: ${ac_type}_${agent.ppo_cfg.adv_compute_mode}
  notes: null

initialization:
  path_to_ckpt: null
  load_denoiser: True
  load_rew_end_model: True
  load_actor_critic: True

checkpointing:
  save_agent_every: 2000
  num_to_keep: 11  # number of checkpoints to keep, use null to disable

collection:
  train:
    num_envs: 1
    epsilon: 0.01
    num_steps_total: 100000
    first_epoch:
      min: 10000
      max: 5000  # null: no maximum
      threshold_rew: 10
    steps_per_epoch: 5000
  test:
    num_envs: 1
    num_episodes: 4
    epsilon: 0.0
    num_final_episodes: 100

# FOR VRL3, try 30000(offline) + 3e6(online)
training:
  should: True
  online_max_iter: 3000000 # 3M for VRL3, ppo ; 2e5 for IQL
  bc_actor_warmup_steps: 20000    # 2e4, just for PPO
  bc_critic_warmup_steps: 20000   # 2e4, just for PPO
  offline_steps: 50000     # 5w for VRL3, ppo ; 2e5 for IQL

evaluation:
  should: True
  every: 5
  every_iter: 20000
  eval_times: 25

actor_critic:
  training:
    sample_weights: null
    batch_size: 2048
    grad_acc_steps: 1
    start_after_epochs: 0
    steps_first_epoch: 400
    steps_per_epoch: 400
    lr_warmup_steps: 100
    max_grad_norm: 0.5

  actor_critic_loss:
    _target_: models.rl_agent.ppo_agent.ActorCriticLossConfig
    backup_every: 15
    gamma: 0.985
    lambda_: 0.95
    weight_value_loss: 1.0
    weight_entropy_loss: 0.001
    weight_bc_loss: 1.0
    is_continuous_action: true


