defaults:
  - _self_
  - env: maniskill
  - agent: metaworld

debug_mode: false
seed: 0
task: PushCube-v1

use_trunk: True          ## can not be modified !!!!!!!!!!!!

is_sparse_reward: True
is_whole_traj: True

save_data: False
save_reward_data: False
only_bc: False
img_size: 128

train_with_bc: True
frame_stack: 1

num_until_update: 4000

## for maniskill
control_mode: pd_ee_delta_pose # Options: pd_joint_delta_pos(8维), pd_ee_delta_pose(7维), pd_ee_delta_pos(4维)
## end for maniskill

# for discrete action, set is_discrete_action to True
bin: 21

train_with_offline_data_mode: ${ac_type}       # rlpd is sepearate 2 buffer, vrl3 is use 1 buffer, drqv2 is not use, and drqv2offline is only use offliendata

hydra:
  run:
    dir: outputs/${now:%Y.%m.%d}/${now:%H%M%S}_${task}_${wandb.group}_${wandb.name}
  job:
    chdir: True  # Don't change the working directory to the output folder

common:
  devices: [6]  # int, list of int, cpu, or all 
  seed: ${seed}
  resume: False # do not modify, set by scripts/resume.sh only.

base_dir: /data/home/jzn/workspace/Off2On-Visual

ac_type: ppo  # Optional[ppo, bc, iql]
expert_rb_dir: ${base_dir}/data/${task}_${img_size}/expert_rb_with_reward_${control_mode}.pkl
traj_num: 50

bc_ckpt_dir: ${base_dir}/ckpt/${task}_${ac_type}_2D_${agent.ppo_cfg.adv_compute_mode}/bc_seed${seed}_${traj_num}traj_${control_mode}.pth

wandb:
  mode: online
  project: offline2online-metaworld_${task}
  entity: jzndd1
  name: seed${seed}_bs${actor_critic.training.batch_size}_minibs${agent.ppo_cfg.mini_batch_size}
  group: ${ac_type}_${agent.ppo_cfg.adv_compute_mode}_wholetraj_usebc_use${traj_num}traj
  notes: null

initialization:
  path_to_ckpt: null
  load_denoiser: True
  load_rew_end_model: True
  load_actor_critic: True

checkpointing:
  save_agent_every: 2000
  num_to_keep: 11  # number of checkpoints to keep, use null to disable

collection:
  train:
    num_envs: 1
    epsilon: 0.01
    num_steps_total: 100000
    first_epoch:
      min: 10000
      max: 5000  # null: no maximum
      threshold_rew: 10
    steps_per_epoch: 5000
  test:
    num_envs: 1
    num_episodes: 4
    epsilon: 0.0
    num_final_episodes: 100

# FOR VRL3, try 30000(offline) + 3e6(online)
training:
  should: True
  online_max_iter: 1500000 # 1.1M for VRL3, ppo ; 2e5 for IQL
  bc_actor_warmup_steps: 20000    # 2e4, just for PPO
  bc_critic_warmup_steps: 20000   # 2e4, just for PPO
  offline_steps: 50000     # 5w for VRL3, ppo ; 2e5 for IQL

evaluation:
  should: True
  every: 20
  every_iter: 100000
  eval_times: 100

actor_critic:
  training:
    sample_weights: null
    batch_size: 2048
    grad_acc_steps: 1
    start_after_epochs: 0
    steps_first_epoch: 400
    steps_per_epoch: 400
    lr_warmup_steps: 100
    max_grad_norm: 0.5

  actor_critic_loss:
    _target_: models.rl_agent.ppo_agent.ActorCriticLossConfig
    backup_every: 15
    gamma: 0.985
    lambda_: 0.95
    weight_value_loss: 1.0
    weight_entropy_loss: 0.0
    weight_bc_loss: 0.1
    is_continuous_action: true


